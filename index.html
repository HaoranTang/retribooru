<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	sup {
	    vertical-align: super;
	    font-size: medium;
        }

	.column2 {
	  float: left;
	  width: 50%;
	  padding: 0px;
	}
	.column {
	  float: left;
	  width: 25%;
	  padding: 0px;
	}
	.row::after {
	  content: "";
	  clear: both;
	  display: table;
	}
</style>

<html>
<head>
	<title>RetriBooru Dataset</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-ZYJJ5WFBF3');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:48px">Retrieving Conditions from Reference Images for Diffusion Models</span>

		<br>
		
		<table align=center width=1000px>
			<table align=center width=1100px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Haoran Tang<sup>12&#8224;</sup></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px">Xin Zhou<sup>1&#8224;</sup></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Jieren Deng<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Zhihong Pan<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px">Hao Tian<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Pratik Chaudhari<sup>2</sup></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=900px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:20px"><sup>1</sup>Baidu USA</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:20px"><sup>2</sup>University of Pennsylvania</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:20px"><sup>&#8224;</sup>Equal Contribution</span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=360px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Arxiv]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/HaoranTang/retribooru'>[GitHub]</a></span><br>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='./resources/bibtex.txt'>[Bibtex]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=800px>
					<center>
						<img class="round" style="width:600px" src="./resources/teaser_v1.png"/>
						<br>
						Top: an illustration of a character clustered into two clothing identities. 
						Middle: top 15 characters and top 30 cloth tags by count, where "Hakurei Reimu" has the maximum count of 3274, and "skirt" is tagged most frequently. 
						Bottom: Four training samples of concept composition, where for each sample we take four different body parts as reference images, and learn to compose them for the augmented generation.
					</center>
				</td>
			</tr>
		</table>
<!-- 		<table align=center width=850px>
			<tr>
				<td>
					This was a template originally made for <a href="http://richzhang.github.io/colorization/">Colorful Image Colorization</a>. The code can be found in this <a href="https://github.com/richzhang/webpage-template">repository</a>.
				</td>
			</tr>
		</table> -->
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Recent diffusion-based subject driven generative methods have enabled image generations with good fidelity for specific objects or human portraits.  
				However, to achieve better versatility for applications, we argue that not only improved datasets and evaluations are desired, but also more careful methods to retrieve only relevant information from conditional images are anticipated.
				To this end, we propose an anime figures dataset RetriBooru-V1, with enhanced identity and clothing labels. We state new tasks enabled by this dataset, and introduce a new diversity metric to measure success in completing these tasks, 
				quantifying the flexibility of image generations. We establish an RAG-inspired baseline method, designed to retrieve precise conditional information from reference images. 
				Then, we compare with current methods on existing task to demonstrate the capability of the proposed method. Finally, we provide baseline experiment results on new tasks, and conduct ablation studies on the possible structural choices.
			</td>
		</tr>
	</table>
	<br>

	<hr>
<!-- 	<center><h1>Presentation</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://photos.onedrive.com/share/D27461EA7AC8629C!386848?cid=D27461EA7AC8629C&resId=D27461EA7AC8629C!386848&authkey=!APSjIFTk6g95MiI&ithint=video&e=deib1d" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>
	<hr> -->

<!-- 	<center><h1>Destroying Spatial Inductive Bias via Data Corruptions</h1></center>
	<table align=center width=800px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/vis_cropped.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					We corrupt the spatial inductie bias from data by destroying its structural information in two ways:
					<ul>
					  <li>Global shuffling: Divide image into N&times;N patches of patch size P, shuffle all patches</li>
					  <li>Local shuffling: Divide image into N&times;N patches of patch size P, shuffle pixels within each patch independently</li>
					</ul>
					
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=900px>
		<tr>
			<td align=center width=900px>
				<center>
					<td><img class="round" style="width:900px" src="./resources/tabs.jpg"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=900px>
		<tr>
			<td><a href=""><img class="round" style="width:459px" src="./resources/exp.png"/></a></td>
			<td>
					Pretraining and Downstream tasks with corruptions lead to consistent observations, which hold for different datasets, architectures, etc.
					<ul>
					  <li>CL relies more on spatial inductive bias than SL; CL relies more on global spatial inductive bias than local</li>
					  <li>Well pretrained CLs with spatial bias are more robust to patch shuffling than SL on Downstream tasks</li>
					</ul>
					
			</td>
		</tr>
	</table>

	<table align=center width=800px>
		<center><h1>Empirical Analysis from Feature Space</h1></center>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/vis_attn_mod-1.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<center>Grad-CAM maps from learned representations</center>
	<br>
	<table align=center width=800px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/unif-1.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<center>Class-wise feature uniformity throughout training (black represents overal feature space)</center>
	<br>
	<div class="row">
		<div class="column">
	    		<img src="./resources/unif_ori-1.png" alt="ori" style="width:100%">
	  	</div>
	  	<div class="column">
	    		<img src="./resources/unif_defocusblur-1.png" alt="def" style="width:100%">
	  	</div>
	  	<div class="column">
	    		<img src="./resources/unif_glo_4-1.png" alt="glo" style="width:100%">
	  	</div>
		<div class="column">
	    		<img src="./resources/unif_loc_8-1.png" alt="loc" style="width:100%">
	  	</div>
	</div>
	<center>t-SNE visualizations</center>
	<br> -->

	<hr>
	<table align=center width=900px>
		<tr>
			<td width=900px>
				<center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</center>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

