<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	sup {
	    vertical-align: super;
	    font-size: medium;
        }

	.column2 {
	  float: left;
	  width: 50%;
	  padding: 0px;
	}
	.column {
	  float: left;
	  width: 25%;
	  padding: 0px;
	}
	.row::after {
	  content: "";
	  clear: both;
	  display: table;
	}
	#dataset-comparison {
	  width: 100%;
	  border-collapse: collapse;
	  margin: 10px 0;
	  font-family: Arial, sans-serif;
	}
	#dataset-comparison caption {
	  caption-side: top;
	  font-size: 16px;
	  font-weight: bold;
	  margin: 5px;
	}
	#dataset-comparison th, #dataset-comparison td {
	  border: 1px solid black;
	  padding: 8px;
	  text-align: center;
	}
	#dataset-comparison th {
	  background-color: #f2f2f2;
	}
</style>

<html>
<head>
	<title>RetriBooru</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-ZYJJ5WFBF3');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:48px">RetriBooru: Leakage-Free Retrieval of Conditions from Reference Images for Subject-Driven Generation</span>

		<br>
		
		<table align=center width=1000px>
			<table align=center width=1100px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Haoran Tang<sup>12</sup></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Jieren Deng<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Zhihong Pan<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px">Hao Tian<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Pratik Chaudhari<sup>2</sup></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px">Xin Zhou<sup>1</sup></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=600px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:20px"><sup>1</sup>Baidu USA</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:20px"><sup>2</sup>University of Pennsylvania</span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=360px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2312.02521'>[Arxiv]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/HaoranTang/retribooru'>[GitHub]</a></span><br>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='./resources/bibtex.txt'>[Bibtex]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=1000px>
			<tr>
				<td width=1000px>
					<center>
						<img class="round" style="width:1000px" src="./resources/teaser_v3.png"/>
						<br>
						<br>
						Training the proposed concept composition task on our RetriBooru dataset. 
						Different concepts to retrieve are specified in texts and passed to the retrieval encoder, 
						which learns only from characteristic information to compose the output, guiding generation 
						with text prompts in the U-Net.
					</center>
				</td>
			</tr>
		</table>
	</center>
	
	<hr>

	<table align=center width=900px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Diffusion-based methods have demonstrated remarkable capabilities in generating a diverse array of high-quality images, 
				sparking interests for styled avatars, virtual try-on, and more. Previous methods use the same reference image as the 
				target. An overlooked aspect is the leakage of the target's spatial information, style, etc. from the reference, 
				harming the generated diversity and causing shortcuts. However, this approach continues as widely available datasets 
				usually consist of single images not grouped by identities, and it is expensive to recollect large-scale same-identity 
				data. Moreover, existing metrics adopt decoupled evaluation on text alignment and identity preservation, which fail at 
				distinguishing between balanced outputs and those that over-fit to one aspect. 
				In this paper, we propose a multi-level, same-identity dataset RetriBooru, which groups anime characters by both face 
				and cloth identities. RetriBooru enables adopting reference images of the same character and outfits as the target, 
				while keeping flexible gestures and actions. We benchmark previous methods on our dataset, and demonstrate the 
				effectiveness of training with a reference image different from target (but same identity). We introduce a new concept 
				composition task, where the conditioning encoder learns to retrieve different concepts from several reference images, 
				and modify a baseline network RetriNet for the new task. Finally, we introduce a novel class of metrics named Similarity 
				Weighted Diversity (SWD), to measure the overlooked diversity and better evaluate the alignment between similarity and 
				diversity.
			</td>
		</tr>
	</table>

	<hr>

	<table align=center width=900px>
		<center><h1>RetriBooru Dataset</h1></center>
		<tr>
			<td width=900px>
				<center>
					<img class="round" style="width:900px" src="./resources/retribooru_annot.png"/>
					<br>
					Details of RetriBooru dataset. Left: annotations of an individual sample. Right: distributions 
					of lengths of the ``similar'' lists, top-15 characters with most appearances, and top-30 cloth tags.
					<br>
					<br>
					In order to train a referenced generation model
					for a given concept, we need to have a concepts-labeled dataset. The reason is
					that training on the same image both as the reference and the target risks leaking
					extra information such as style and geometric bias for "shortcut" convergence. Existing datasets are 
					lacking in size, types of identities, single-tasked and may be harder to iterate on than anime (faces, for
					example) as shown in the below table. We use InstructBLIP-Vicuna 7B with simple yet strict
					heuristics to aid our annotations of clothing identities, based on Danbooru 2019 Figures datase. 
					Please refer to our paper for detailed dataset processing and construction, 
					and we will release RetriBooru soon on HuggingFace&#129303;.
					<br>
					<table id="dataset-comparison">
					    <caption>Comparisons with datasets used in other literature.</caption>
					    <tr>
					        <th>Dataset</th>
					        <th>Size</th>
					        <th>Category</th>
					        <th>Multi-images per identity</th>
					        <th>Concept composition</th>
					        <th>Data source</th>
					    </tr>
					    <tr>
					        <td>DreamBooth</td>
					        <td>&le; 180</td>
					        <td>Objects</td>
					        <td>✔</td>
					        <td></td>
					        <td>Web</td>
					    </tr>
					    <tr>
					        <td>BLIP-Diffusion</td>
					        <td>129M</td>
					        <td>Objects</td>
					        <td></td>
					        <td></td>
					        <td>Datasets Mixture</td>
					    </tr>
					    <tr>
					        <td>FastComposer</td>
					        <td>70K</td>
					        <td>Human Faces</td>
					        <td></td>
					        <td>✔</td>
					        <td>FFHQ-wild</td>
					    </tr>
					    <tr>
					        <td>CustomConcept101</td>
					        <td>642</td>
					        <td>Objects and Faces</td>
					        <td>✔</td>
					        <td>✔</td>
					        <td>Web</td>
					    </tr>
					    <tr>
					        <td><strong>Ours</strong></td>
					        <td>116K</td>
					        <td>Anime Figures</td>
					        <td>✔</td>
					        <td>✔</td>
					        <td>Danbooru</td>
					    </tr>
					</table>
				</center>
			</td>
		</tr>
	</table>

	<hr>
	
	<table align=center width=900px>
		<center><h1>RetriNet Architecture</h1></center>
		<tr>
			<td width=900px>
				<center>
					<img class="round" style="width:900px" src="./resources/arch_figure.png"/>
					<br>
					<br>
					An illustration of concept composition task on RetriBooru using RetriNet. We pass reference concepts into the 
					retrieval encoder to encode precisely only the relevant information. We pass noisy target image and prompt made 
					with Danbooru tags to a copy of denoising U-Net. Note that we process reference images, texts, and time with 
					corresponding frozen encoders, and we have frozen encoder and middle blocks of SD. We designate the embeddings 
					derived from target images and prompts as Query (Q), while the embeddings from reference images and text serve as 
					Key and Values (KV). Following the cross attention layers are zero-convolution layers.
				</center>
			</td>
		</tr>
	</table>

<!-- 	<hr>

	<table align=center width=900px>
		<center><h1>Similarity Weighted Diversity</h1></center>
		<tr>
			<td width=900px>
				<center>
					We provide a class of metrics to combine similarity and diversity metrics and argue that this 
					combination is crucial for referenced generation. In short, we encourage generation to have maximal diversity of facial and body details 
					while preserving identity. See our paper for detailed formulations. For application, we evaluate diversity by asking VQA model (InstructBLIP-Vicuna 7B) 
					about the image details, and evaluate similarity by CLIP scores and DeepFace-L2 if human faces are evaluated. Our metric will also benefit from continued 
					improvements in the precision of similarity and diversity measurements.
				</center>
			</td>
		</tr>
	</table>

	<br>
	
	<table align=center width=900px>
		<tr>
			<td width=900px>
				<center>
					<b>Example generations given prompt and reference images.</b>
				</center>
				<br>
				<center>
					<img class="round" style="width:900px" src="./resources/anime_demo_v3_p0.png"/>
					<br>
					<br>
					<img class="round" style="width:900px" src="./resources/anime_demo_v3_p3.png"/>
				</center>
			</td>
		</tr>
	</table> -->

	<hr>
	<table align=center width=900px>
		<tr>
			<td width=900px>
				<center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</center>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

