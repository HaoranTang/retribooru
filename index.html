<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	sup {
	    vertical-align: super;
	    font-size: medium;
        }

	.column2 {
	  float: left;
	  width: 50%;
	  padding: 0px;
	}
	.column {
	  float: left;
	  width: 25%;
	  padding: 0px;
	}
	.row::after {
	  content: "";
	  clear: both;
	  display: table;
	}
	#dataset-comparison {
	  width: 100%;
	  border-collapse: collapse;
	  margin: 10px 0;
	  font-family: Arial, sans-serif;
	}
	#dataset-comparison caption {
	  caption-side: top;
	  font-size: 16px;
	  font-weight: bold;
	  margin: 5px;
	}
	#dataset-comparison th, #dataset-comparison td {
	  border: 1px solid black;
	  padding: 8px;
	  text-align: center;
	}
	#dataset-comparison th {
	  background-color: #f2f2f2;
	}
</style>

<html>
<head>
	<title>Retriving Conditions from Reference</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-ZYJJ5WFBF3');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:48px">Retrieving Conditions from Reference Images for Diffusion Models</span>

		<br>
		
		<table align=center width=1000px>
			<table align=center width=1100px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Haoran Tang<sup>12&#8224;</sup></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px">Xin Zhou<sup>1&#8224;</sup></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Jieren Deng<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Zhihong Pan<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px">Hao Tian<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">Pratik Chaudhari<sup>2</sup></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=900px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:20px"><sup>1</sup>Baidu USA</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:20px"><sup>2</sup>University of Pennsylvania</span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:20px"><sup>&#8224;</sup>Equal Contribution</span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=360px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2312.02521'>[Arxiv]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/HaoranTang/retribooru'>[GitHub]</a></span><br>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='./resources/bibtex.txt'>[Bibtex]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=1000px>
			<tr>
				<td width=1000px>
					<center>
						<img class="round" style="width:1000px" src="./resources/teaser_v3.png"/>
						<br>
						<br>
						An illustration of the proposed task of concept composition. Face identity
						information are retrieved and composed with clothing information.
					</center>
				</td>
			</tr>
		</table>
	</center>
	
	<hr>

	<table align=center width=900px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Newly developed diffusion-based techniques have showcased
				phenomenal abilities in producing a wide range of high-quality images,
				sparking considerable interest in various applications. A prevalent scenario 
				is to generate new images based on a subject from reference images.
				This subject could be face identity for styled avatars, body and clothing 
				for virtual try-on and so on. Satisfying this requirement is evolving
				into a field called <i>Subject-Driven Generation</i>. In this paper, we consider
				Subject-Driven Generation as a unified retrieval problem with diffusion
				models. We introduce a novel diffusion model architecture, named <b>RetriNet</b>, 
				designed to address and solve these problems by retrieving subject
				attributes from reference images precisely, and filter out irrelevant information. 
				RetriNet demonstrates impressive performance when compared
				to existing state-of-the-art approaches in face generation. We further
				propose a research and iteration friendly dataset, <b>RetriBooru</b>, to study a
				more difficult problem, concept composition. Finally, to better evaluate
				alignment between similarity and diversity or measure diversity that have
				been previously unaccounted for, we introduce a novel class of metrics
				named Similarity Weighted Diversity (SWD).
			</td>
		</tr>
	</table>

	<hr>

	<table align=center width=900px>
		<center><h1>RetriNet Architecture</h1></center>
		<tr>
			<td width=900px>
				<center>
					<img class="round" style="width:900px" src="./resources/arch_figure.png"/>
					<br>
					<br>
					RetriNet architecture. At each timestep, we pass reference images and reference
					concepts into the retrieval encoder to encode precisely only the relevant information.
					We pass noisy target image and prompt made directly with Danbooru tags to a copy of
					standard stable diffusion UNet. Note that we process reference images, texts, and time
					with corresponding frozen encoders, and we have frozen encoder and middle blocks of
					SD. We connect layer outputs of retrieval encoder to SD decoder via our conjunction
					network consisted of cross-attentions and zero-convolutions.
				</center>
			</td>
		</tr>
	</table>

	<hr>

	<table align=center width=900px>
		<center><h1>RetriBooru Dataset</h1></center>
		<tr>
			<td width=900px>
				<center>
					In order to train a referenced generation model
					for a given concept, we need to have a concept labeled dataset. The reason is
					that training on the same image both as the reference and the target risk leaking
					extra information such as style. Existing datasets are lacking in size, types of
					identities, single-tasked and may be harder to iterate on than anime (faces, for
					example) as shown in the below table. We use InstructBLIP-Vicuna 7B with simple yet strict
					heuristics to aid our annotations of clothing identities, based on Danbooru 2019 Figures datase. 
					Please refer to our paper for detailed dataset processing and construction, 
					and we will release RetriBooru soon on HuggingFace&#129303;.
					<br>
					<table id="dataset-comparison">
					    <caption>Comparisons with datasets used in other literature.</caption>
					    <tr>
					        <th>Dataset</th>
					        <th>Size</th>
					        <th>Category</th>
					        <th>Multi-images per identity</th>
					        <th>Concept composition</th>
					        <th>Data source</th>
					    </tr>
					    <tr>
					        <td>DreamBooth</td>
					        <td>&le; 180</td>
					        <td>Objects</td>
					        <td>✔</td>
					        <td></td>
					        <td>Web</td>
					    </tr>
					    <tr>
					        <td>BLIP-Diffusion</td>
					        <td>129M</td>
					        <td>Objects</td>
					        <td></td>
					        <td></td>
					        <td>Datasets Mixture</td>
					    </tr>
					    <tr>
					        <td>FastComposer</td>
					        <td>70K</td>
					        <td>Human Faces</td>
					        <td></td>
					        <td>✔</td>
					        <td>FFHQ-wild</td>
					    </tr>
					    <tr>
					        <td>CustomConcept101</td>
					        <td>642</td>
					        <td>Objects and Faces</td>
					        <td>✔</td>
					        <td>✔</td>
					        <td>Web</td>
					    </tr>
					    <tr>
					        <td><strong>Ours</strong></td>
					        <td>116K</td>
					        <td>Anime Figures</td>
					        <td>✔</td>
					        <td>✔</td>
					        <td>Danbooru</td>
					    </tr>
					</table>
				</center>
			</td>
		</tr>
	</table>

	<hr>

	<table align=center width=900px>
		<center><h1>Similarity Weighted Diversity</h1></center>
		<tr>
			<td width=900px>
				<center>
					We provide a class of metrics to combine similarity and diversity metrics and argue that this 
					combination is crucial for referenced generation. In short, we encourage generation to have maximal diversity of facial and body details 
					while preserving identity. See our paper for detailed formulations. For application, we evaluate diversity by asking VQA model (InstructBLIP-Vicuna 7B) 
					about the image details, and evaluate similarity by CLIP scores and DeepFace-L2 if human faces are evaluated. Our metric will also benefit from continued 
					improvements in the precision of similarity and diversity measurements.
				</center>
			</td>
		</tr>
	</table>

<!-- 	<center><h1>Destroying Spatial Inductive Bias via Data Corruptions</h1></center>
	<table align=center width=800px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/vis_cropped.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					We corrupt the spatial inductie bias from data by destroying its structural information in two ways:
					<ul>
					  <li>Global shuffling: Divide image into N&times;N patches of patch size P, shuffle all patches</li>
					  <li>Local shuffling: Divide image into N&times;N patches of patch size P, shuffle pixels within each patch independently</li>
					</ul>
					
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=900px>
		<tr>
			<td align=center width=900px>
				<center>
					<td><img class="round" style="width:900px" src="./resources/tabs.jpg"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=900px>
		<tr>
			<td><a href=""><img class="round" style="width:459px" src="./resources/exp.png"/></a></td>
			<td>
					Pretraining and Downstream tasks with corruptions lead to consistent observations, which hold for different datasets, architectures, etc.
					<ul>
					  <li>CL relies more on spatial inductive bias than SL; CL relies more on global spatial inductive bias than local</li>
					  <li>Well pretrained CLs with spatial bias are more robust to patch shuffling than SL on Downstream tasks</li>
					</ul>
					
			</td>
		</tr>
	</table>

	<table align=center width=800px>
		<center><h1>Empirical Analysis from Feature Space</h1></center>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/vis_attn_mod-1.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<center>Grad-CAM maps from learned representations</center>
	<br>
	<table align=center width=800px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/unif-1.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<center>Class-wise feature uniformity throughout training (black represents overal feature space)</center>
	<br>
	<div class="row">
		<div class="column">
	    		<img src="./resources/unif_ori-1.png" alt="ori" style="width:100%">
	  	</div>
	  	<div class="column">
	    		<img src="./resources/unif_defocusblur-1.png" alt="def" style="width:100%">
	  	</div>
	  	<div class="column">
	    		<img src="./resources/unif_glo_4-1.png" alt="glo" style="width:100%">
	  	</div>
		<div class="column">
	    		<img src="./resources/unif_loc_8-1.png" alt="loc" style="width:100%">
	  	</div>
	</div>
	<center>t-SNE visualizations</center>
	<br> -->

	<hr>
	<table align=center width=900px>
		<tr>
			<td width=900px>
				<center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</center>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

